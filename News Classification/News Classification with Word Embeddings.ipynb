{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">NLP :Text Classification USing countvectorization,tf-idf and Gensim Word Embeddings</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fake news refers to misinformation or disinformation in the country which is spread through word of mouth and more recently through digital communication such as What's app messages, social media posts, etc.\n",
    "\n",
    "- Fake news spreads faster than real news and creates problems and fear among groups and in society.\n",
    "\n",
    "- We are going to address these problems using classical NLP techniques and going to classify whether a given message/ text is **Real or Fake Message**.\n",
    "\n",
    "- We will use **glove embeddings** from spacy which is trained on massive wikipedia dataset to pre-process and text vectorization and apply different classification algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Project\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This Jupyter Notebook documents a comprehensive text classification project. The goal of the project is to develop and evaluate machine learning models capable of distinguishing between fake and real news articles. The dataset used for this task is sourced from the \"fake_or_real_news\" dataset, comprising textual information and corresponding labels indicating the authenticity of each news article.\n",
    "\n",
    "The project encompasses various stages, including data loading, text preprocessing, feature engineering through vectorization techniques, model training, evaluation, and hyperparameter tuning. Both traditional machine learning approaches, such as Multinomial Naive Bayes and Gradient Boosting, and advanced techniques like word embeddings, are explored and compared.\n",
    "\n",
    "### Project Workflow\n",
    "\n",
    "1. **Loading the Dataset:**\n",
    "   - The dataset is loaded using Pandas to gain an understanding of its structure.\n",
    "\n",
    "2. **Text Preprocessing:**\n",
    "   - Textual data is preprocessed to enhance the quality of features for model training. Techniques include lowercase conversion, removal of special characters and stop words, tokenization, and optional stemming.\n",
    "\n",
    "3. **Label Encoding:**\n",
    "   - Labels indicating \"FAKE\" or \"REAL\" are encoded numerically to facilitate model training.\n",
    "\n",
    "4. **Vectorization:**\n",
    "   - The textual data is converted into numerical vectors using CountVectorizer, TfidfVectorizer, and word embeddings (Word2Vec and Spacy).\n",
    "\n",
    "5. **Model Training and Evaluation:**\n",
    "   - Models, including Multinomial Naive Bayes and Gradient Boosting, are trained and evaluated using different vectorization techniques.\n",
    "\n",
    "6. **Word Embeddings with Gensim and Spacy:**\n",
    "   - Pre-trained word embeddings models are used to create numerical representations for words.\n",
    "\n",
    "7. **Word Embedding-Based Model (Gradient Boosting):**\n",
    "   - A Gradient Boosting model is trained using vectors obtained from word embeddings.\n",
    "\n",
    "8. **Hyperparameter Tuning (Grid Search):**\n",
    "   - Hyperparameter tuning is performed using Grid Search to optimize model performance.\n",
    "\n",
    "9. **Analysis and Comparison:**\n",
    "   - Results are analyzed and compared, considering accuracy, precision, recall, and F1-score.\n",
    "\n",
    "10. **Conclusion:**\n",
    "    - Key findings and model performances are summarized, and recommendations for further improvement are provided.\n",
    "\n",
    "11. **Additional Recommendations:**\n",
    "    - Suggestions for future enhancements, such as incorporating more data and exploring deep learning approaches, are outlined.\n",
    "\n",
    "The following sections detail each step of the project and present the outcomes and analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Data Exploration and Overview\n",
    "\n",
    "The dataset is collected from 'https://www.kaggle.com/datasets/hassanamin/textdb3/code'. \n",
    "\n",
    "- This data consists of two columns.\n",
    "        - Text\n",
    "        - label\n",
    "- Text is the statements or messages regarding a particular event/situation.\n",
    "\n",
    "- label feature tells whether the given text is Fake or Real.\n",
    "\n",
    "- As there are only 2 classes, this problem comes under the **Binary Classification.**\n",
    "The dataset \"fake_or_real_news.csv\" is loaded into a Pandas DataFrame named 'df'. The DataFrame contains 6335 entries and consists of four columns:\n",
    "\n",
    "1. **Unnamed: 0:** An integer column serving as an index.\n",
    "2. **title:** The title of the news article.\n",
    "3. **text:** The content or body of the news article.\n",
    "4. **label:** The label indicating whether the news is classified as \"FAKE\" or \"REAL\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fake News Classifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('fake_or_real_news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  6335 non-null   int64 \n",
      " 1   title       6335 non-null   object\n",
      " 2   text        6335 non-null   object\n",
      " 3   label       6335 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 198.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has no missing values, and the data types for each column are appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Text Preprocessing Function\n",
    "\n",
    "A text preprocessing function, named `preprocess_text`, has been defined. This function applies several common text preprocessing steps to clean and prepare the text data for classification. The steps include:\n",
    "\n",
    "1. **Convert to lowercase:** All text is converted to lowercase to ensure uniformity.\n",
    "\n",
    "2. **Remove special characters, numbers, and extra whitespaces:** Regular expressions are used to eliminate any non-alphabetic characters, numbers, and unnecessary whitespaces.\n",
    "\n",
    "3. **Tokenization:** The text is tokenized into individual words or tokens using the `word_tokenize` function from the NLTK library.\n",
    "\n",
    "4. **Remove stop words:** Stop words (common words like 'the', 'and', 'is') are removed from the tokenized text to focus on more meaningful words.\n",
    "\n",
    "5. **Stemming (optional):** An optional step is included for stemming, reducing words to their root form using the Porter Stemmer.\n",
    "\n",
    "6. **Join tokens back into a single string:** The processed tokens are joined back into a single string, forming the preprocessed text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "REAL    3171\n",
       "FAKE    3164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, numbers, and extra whitespaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply the preprocessing function to the 'text' column in the DataFrame\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "# changing label 'Fake' to 0 and Real to 1\n",
    "df['label_num'] = df['label'].map({'FAKE' : 0, 'REAL': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>daniel greenfield shillman journal fellow free...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>googl pinterest digg linkedin reddit stumbleup...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>us secretari state john f kerri said monday st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>kayde king kaydeek novemb lesson tonight dem l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>primari day new york frontrunn hillari clinton...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>4490</td>\n",
       "      <td>State Department says it can't find emails fro...</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>state depart told republican nation committe c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>8062</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>p pb stand plutocrat pentagon post oct wikimed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>8622</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligarc...</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>antitrump protest tool oligarchi reform alway ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>4021</td>\n",
       "      <td>In Ethiopia, Obama seeks progress on peace, se...</td>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>addi ababa ethiopia presid obama conven meet l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>4330</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>jeb bush suddenli attack trump here matter jeb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6335 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "0           8476                       You Can Smell Hillary’s Fear   \n",
       "1          10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2           3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3          10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4            875   The Battle of New York: Why This Primary Matters   \n",
       "...          ...                                                ...   \n",
       "6330        4490  State Department says it can't find emails fro...   \n",
       "6331        8062  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
       "6332        8622  Anti-Trump Protesters Are Tools of the Oligarc...   \n",
       "6333        4021  In Ethiopia, Obama seeks progress on peace, se...   \n",
       "6334        4330  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
       "\n",
       "                                                   text label  \\\n",
       "0     Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1     Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2     U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3     — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4     It's primary day in New York and front-runners...  REAL   \n",
       "...                                                 ...   ...   \n",
       "6330  The State Department told the Republican Natio...  REAL   \n",
       "6331  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...  FAKE   \n",
       "6332   Anti-Trump Protesters Are Tools of the Oligar...  FAKE   \n",
       "6333  ADDIS ABABA, Ethiopia —President Obama convene...  REAL   \n",
       "6334  Jeb Bush Is Suddenly Attacking Trump. Here's W...  REAL   \n",
       "\n",
       "                                         processed_text  label_num  \n",
       "0     daniel greenfield shillman journal fellow free...          0  \n",
       "1     googl pinterest digg linkedin reddit stumbleup...          0  \n",
       "2     us secretari state john f kerri said monday st...          1  \n",
       "3     kayde king kaydeek novemb lesson tonight dem l...          0  \n",
       "4     primari day new york frontrunn hillari clinton...          1  \n",
       "...                                                 ...        ...  \n",
       "6330  state depart told republican nation committe c...          1  \n",
       "6331  p pb stand plutocrat pentagon post oct wikimed...          0  \n",
       "6332  antitrump protest tool oligarchi reform alway ...          0  \n",
       "6333  addi ababa ethiopia presid obama conven meet l...          1  \n",
       "6334  jeb bush suddenli attack trump here matter jeb...          1  \n",
       "\n",
       "[6335 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing function is applied to the 'text' column in the DataFrame, and the results are stored in a new column named 'processed_text'. This column now contains the preprocessed text, ready for further vectorization and model training. A new column named 'label_num' also created by converting the label 'Fake' to 0 and 'Real' to 1 using label encoding.\n",
    "\n",
    "The displayed DataFrame showcases the original columns ('Unnamed: 0', 'title', 'text', 'label','label_num) along with the newly added  columns, which contains the preprocessed text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer for text classification\n",
    "A CountVectorizer object, 'count_vectorizer', has been employed to transform the training and test data into matrices of token counts. This process converts the text data into a numerical format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aab' 'aachen' 'aadmi' 'aae' 'aah' 'aaibal' 'aalia' 'aaliya']\n"
     ]
    }
   ],
   "source": [
    "# Create a series to store the labels: y\n",
    "y = df.label_num\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"processed_text\"],y,test_size=.20,random_state=11)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with countervectorizer\n",
    "#### count_naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8752959747434885\n",
      "[[526  93]\n",
      " [ 65 583]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=[0, 1])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count_gradient_boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88       619\n",
      "           1       0.91      0.83      0.87       648\n",
      "\n",
      "    accuracy                           0.87      1267\n",
      "   macro avg       0.88      0.87      0.87      1267\n",
      "weighted avg       0.88      0.87      0.87      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(count_train, y_train)\n",
    "\n",
    "# Get the predictions for the test data\n",
    "y_pred = clf.predict(count_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count_vector Model Evaluation and Results\n",
    "\n",
    "### Multinomial Naive Bayes Classifier (Count Vectorization)\n",
    "\n",
    "#### Results\n",
    "\n",
    "The Multinomial Naive Bayes classifier, trained using Count Vectorization, achieved an accuracy score of approximately 87.5% on the test data. The confusion matrix further breaks down the model's performance:\n",
    "\n",
    "|            | Predicted Fake (1) | Predicted Real (0) |\n",
    "|------------|---------------------|--------------------|\n",
    "| Actual Fake (1) | 583                 | 65                 |\n",
    "| Actual Real (0) | 93                  | 526                |\n",
    "\n",
    "The confusion matrix shows that the model performed well in correctly classifying both fake and real news articles. However, it had a slightly higher number of false positives (65) compared to false negatives (93).\n",
    "\n",
    "### Gradient Boosting Classifier (Count Vectorization)\n",
    "\n",
    "#### Results\n",
    "\n",
    "The Gradient Boosting Classifier, trained using Count Vectorization, achieved an accuracy of approximately 87% on the test data. The classification report provides a more detailed breakdown of the model's performance:\n",
    "\n",
    "|              | Precision | Recall | F1-Score | Support |\n",
    "|--------------|-----------|--------|----------|---------|\n",
    "| Fake (1)      | 0.91      | 0.83   | 0.87     | 648     |\n",
    "| Real (0)      | 0.84      | 0.92   | 0.88     | 619     |\n",
    "| Accuracy     |           |        | 0.87     | 1267    |\n",
    "| Macro Avg    | 0.88      | 0.87   | 0.87     | 1267    |\n",
    "| Weighted Avg | 0.88      | 0.87   | 0.87     | 1267    |\n",
    "\n",
    "The classification report shows that the model performs well in both precision and recall for both classes. The macro and weighted averages further confirm the overall good performance of the Gradient Boosting Classifier.\n",
    "\n",
    "These results indicate promising outcomes for both models in classifying fake and real news articles based on the provided features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer for text classification\n",
    "For an alternative vectorization approach, a TfidfVectorizer object, 'tfidf_vectorizer', has been utilized. This vectorizer not only considers token counts but also incorporates the term frequency-inverse document frequency (TF-IDF) to emphasize important words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aab' 'aachen' 'aadmi' 'aae' 'aah' 'aaibal' 'aalia' 'aaliya']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with tfidf\n",
    "#### tfidf_naive_bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8476716653512234\n",
      "[[448 171]\n",
      " [ 22 626]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf_gradinetboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.85      0.92      0.88       619\n",
      "        REAL       0.92      0.84      0.88       648\n",
      "\n",
      "    accuracy                           0.88      1267\n",
      "   macro avg       0.88      0.88      0.88      1267\n",
      "weighted avg       0.88      0.88      0.88      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(tfidf_train, y_train)\n",
    "\n",
    "# Get the predictions for the test data\n",
    "y_pred = clf.predict(tfidf_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TF-IDF Vectorization Model Evaluation and analysis\n",
    "\n",
    "### Multinomial Naive Bayes Classifier (TF-IDF Vectorization)\n",
    "\n",
    "#### Results\n",
    "\n",
    "The Multinomial Naive Bayes classifier, trained using TF-IDF Vectorization, achieved an accuracy score of approximately 84.8% on the test data. The confusion matrix reveals the following:\n",
    "\n",
    "|               | Predicted Fake (1) | Predicted Real (0) |\n",
    "|---------------|---------------------|---------------------|\n",
    "| Actual Fake (1)| 626                 | 22                  |\n",
    "| Actual Real (0)| 171                 | 448                 |\n",
    "\n",
    "The model demonstrates strong performance in correctly identifying real news articles but exhibits a higher number of false positives compared to false negatives. This may indicate a tendency to misclassify some real articles as fake.\n",
    "\n",
    "### Gradient Boosting Classifier (TF-IDF Vectorization)\n",
    "\n",
    "#### Results\n",
    "\n",
    "The Gradient Boosting Classifier, trained using TF-IDF Vectorization, achieved an accuracy of approximately 88% on the test data. The classification report provides a detailed breakdown of the model's performance:\n",
    "\n",
    "|              | Precision | Recall | F1-Score | Support |\n",
    "|--------------|-----------|--------|----------|---------|\n",
    "| Fake (1)      | 0.92      | 0.84   | 0.88     | 648     |\n",
    "| Real (0)      | 0.85      | 0.92   | 0.88     | 619     |\n",
    "| Accuracy     |           |        | 0.88     | 1267    |\n",
    "| Macro Avg    | 0.88      | 0.88   | 0.88     | 1267    |\n",
    "| Weighted Avg | 0.88      | 0.88   | 0.88     | 1267    |\n",
    "\n",
    "The Gradient Boosting Classifier excels in both precision and recall for both classes. The balanced performance across the two classes suggests that this model effectively distinguishes between fake and real news articles.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Comparing the results of TF-IDF Vectorization with Count Vectorization, both models demonstrate competitive performance. However, the Gradient Boosting Classifier, when trained on TF-IDF vectors, exhibits slightly improved accuracy and balanced precision and recall for both classes. This suggests that TF-IDF features contribute to a more nuanced and discriminative representation of the text data, enhancing the performance of complex models like Gradient Boosting.\n",
    "\n",
    "In conclusion, TF-IDF Vectorization proves to be a valuable feature representation for text classification, contributing to improved model accuracy and balance in performance across different classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8808208366219415\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.89344909234412\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8887134964483031\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8768745067087609\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8713496448303079\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8626677190213102\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.8602999210734017\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8579321231254933\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8555643251775849\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8500394632991318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:629: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:635: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train,y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with tfidf model\n",
    "The results suggest that an alpha value of 0.1 yields the highest accuracy score of approximately 89.3%. Further analysis and experimentation with hyperparameter tuning could be conducted to fine-tune the model for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8808208366219415\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.89344909234412\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8887134964483031\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8768745067087609\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8713496448303079\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8626677190213102\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.8602999210734017\n",
      "\n",
      "Alpha:  0.7000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:629: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:635: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8579321231254933\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8555643251775849\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8500394632991318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train,y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Gradient Boosting Classifier (TF-IDF Vectorization)\n",
    "\n",
    "To optimize the performance of the Gradient Boosting Classifier, a grid search is conducted over a simplified parameter grid. The grid includes variations in the number of trees (n_estimators), learning rate (step size shrinkage), maximum depth of trees (max_depth), minimum samples required to split an internal node (min_samples_split), minimum samples required to be at a leaf node (min_samples_leaf), and the fraction of samples used for fitting individual base learners (subsample).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.87      0.93      0.90       619\n",
      "        REAL       0.93      0.86      0.89       648\n",
      "\n",
      "    accuracy                           0.90      1267\n",
      "   macro avg       0.90      0.90      0.90      1267\n",
      "weighted avg       0.90      0.90      0.90      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# Define a simplified parameter grid for faster search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],                  # Number of trees\n",
    "    'learning_rate': [0.01, 0.1],               # Step size shrinkage\n",
    "    'max_depth': [3, 4],                        # Maximum depth of the individual trees\n",
    "    'min_samples_split': [2, 5],                # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2],                 # Minimum number of samples required to be at a leaf node\n",
    "    'subsample': [0.8, 1.0]                     # Fraction of samples used for fitting the individual base learners\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(tfidf_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(tfidf_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison: Before vs After Hyperparameter Tuning\n",
    "\n",
    "### Multinomial Naive Bayes Classifier (TF-IDF Vectorization)\n",
    "\n",
    "#### Before Hyperparameter Tuning\n",
    "\n",
    "The Multinomial Naive Bayes classifier, trained using TF-IDF Vectorization, achieved an accuracy score of approximately 84.8% on the test data. The confusion matrix revealed some misclassification tendencies, with a higher number of false positives compared to false negatives.\n",
    "\n",
    "#### After Hyperparameter Tuning\n",
    "\n",
    "A grid search was performed to optimize the Multinomial Naive Bayes classifier by varying the smoothing parameter, alpha. The best alpha value found was 0.1, leading to an improved accuracy score of approximately 89.3% on the test data. The tuned model demonstrated better performance, particularly in reducing false positives.\n",
    "\n",
    "### Gradient Boosting Classifier (TF-IDF Vectorization)\n",
    "\n",
    "#### Before Hyperparameter Tuning\n",
    "\n",
    "The Gradient Boosting Classifier, trained using TF-IDF Vectorization, achieved an accuracy of approximately 88% on the test data. The classification report provided a detailed breakdown of the model's performance, indicating balanced precision and recall for both classes.\n",
    "\n",
    "#### After Hyperparameter Tuning\n",
    "\n",
    "A grid search was conducted over a simplified parameter grid to optimize the Gradient Boosting Classifier. The best hyperparameters found were a learning rate of 0.1, max depth of 4, min samples leaf of 2, min samples split of 5, number of estimators of 100, and subsample of 0.8. The tuned model demonstrated improved accuracy of 90% on the test data, showcasing the effectiveness of hyperparameter tuning in enhancing the model's predictive capabilities.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Hyperparameter tuning proved beneficial for both models. The Multinomial Naive Bayes classifier exhibited a significant accuracy improvement after tuning, showcasing the impact of alpha selection on performance. The Gradient Boosting Classifier, while already performing well, further benefited from hyperparameter tuning, demonstrating the potential for fine-tuning complex models to achieve optimal results.\n",
    "\n",
    "Overall, hyperparameter tuning enhances the robustness and effectiveness of these text classification models, providing a more accurate and balanced prediction of fake and real news articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDNq8C5rMEI2"
   },
   "source": [
    "## **News Classification Using Word2Vec Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('fake_or_real_news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3FuwvuFCBaNY",
    "outputId": "cae80a0f-e2de-4944-d3aa-8d59ca014cf9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  label_num  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE          0  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE          0  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL          1  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE          0  \n",
       "4  It's primary day in New York and front-runners...  REAL          1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add the new column which gives a unique number to each of these labels \n",
    "df['label_num'] = df['label'].map({'FAKE' : 0, 'REAL': 1})\n",
    "\n",
    "#check the results with top 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will convert the text into a vector using gensim's word2vec embeddings. \n",
    "#### We will do this in three steps,\n",
    "\n",
    "1. Preprocess the text to remove stop words, punctuations and get lemma for each word\n",
    "2. Get word vectors for each of the words in a pre-processed sentece\n",
    "3. Take a mean of all word vectors to derive the numeric representation of the entire news article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ged-kUXTIdq5"
   },
   "source": [
    "### load gensim and preprocessing\n",
    "Now let's write the function that can do preprocessing and vectorization both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google News Word2vec model from gensim library\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "5-CdpAtmIbjQ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\") # for better performance it is recommended to use en_core_web_lg\n",
    "def preprocess_and_vectorize(text):\n",
    "    # remove stop words and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "        \n",
    "    return wv.get_mean_vector(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the vector size\n",
    "preprocess_and_vectorize(\"What we do in life echoes in eternity\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "iOke8KXkIbmE"
   },
   "outputs": [],
   "source": [
    "# creating a new column that contains the vector of the word after embedding\n",
    "\n",
    "df['vector'] = df['text'].apply(lambda text: preprocess_and_vectorize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.02503515, 0.012944135, -0.0007807008, 0.020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.027095925, 0.010018359, -0.003129262, 0.026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.007521171, 0.016987685, 0.01482916, 0.02073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.026640898, 0.0016970291, -0.005357439, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.017088465, 0.012695167, -0.003302912, -0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  label_num  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE          0   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE          0   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL          1   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE          0   \n",
       "4  It's primary day in New York and front-runners...  REAL          1   \n",
       "\n",
       "                                              vector  \n",
       "0  [0.02503515, 0.012944135, -0.0007807008, 0.020...  \n",
       "1  [0.027095925, 0.010018359, -0.003129262, 0.026...  \n",
       "2  [0.007521171, 0.016987685, 0.01482916, 0.02073...  \n",
       "3  [0.026640898, 0.0016970291, -0.005357439, 0.01...  \n",
       "4  [0.017088465, 0.012695167, -0.003302912, -0.00...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2XGariWM09j"
   },
   "source": [
    "#### **Train-Test splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "OJfmmbxBDYuO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Do the 'train-test' splitting with test size of 20% with random state of 9012 and stratify sampling too\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.vector.values, \n",
    "    df.label_num, \n",
    "    test_size=0.20, # 20% samples will go to test dataset\n",
    "    random_state=9012,\n",
    "    stratify=df.label_num\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08-oivPtM8a5"
   },
   "source": [
    "**Reshaping the X_train and X_test so as to fit for models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOOO4prQi7hg",
    "outputId": "ea31fc23-a63b-42ff-fef5-6e21c038b733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train before reshaping:  (5068,)\n",
      "Shape of X_test before reshaping:  (1267,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train before reshaping: \", X_train.shape)\n",
    "print(\"Shape of X_test before reshaping: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reshaping of `X_train` and 'X_test' from a 1D array  to a 2D array is necessary for compatibility with machine learning models that expect a 2D input format. Each row in the reshaped array represents a document, and the columns represent the features, specifically the elements of the word embedding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train after reshaping:  (5068, 300)\n",
      "Shape of X_test after reshaping:  (1267, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d =  np.stack(X_test)\n",
    "\n",
    "print(\"Shape of X_train after reshaping: \", X_train_2d.shape)\n",
    "print(\"Shape of X_test after reshaping: \", X_test_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5HmfA2gRyQY"
   },
   "source": [
    "### Word Embedding gradient boost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91       633\n",
      "           1       0.93      0.88      0.90       634\n",
      "\n",
      "    accuracy                           0.90      1267\n",
      "   macro avg       0.90      0.90      0.90      1267\n",
      "weighted avg       0.90      0.90      0.90      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#1. creating a GradientBoosting model object\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "#2. fit with all_train_embeddings and y_train\n",
    "clf.fit(X_train_2d, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for all_test_embeddings and store it in y_pred\n",
    "y_pred = clf.predict(X_test_2d)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Q8Sy2iENZr7",
    "outputId": "159d92d5-17bd-4c18-c797-ed049b6db0ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91       633\n",
      "           1       0.93      0.88      0.90       634\n",
      "\n",
      "    accuracy                           0.90      1267\n",
      "   macro avg       0.90      0.90      0.90      1267\n",
      "weighted avg       0.90      0.90      0.90      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#1. creating a GradientBoosting model object\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "#2. fit with all_train_embeddings and y_train\n",
    "clf.fit(X_train_2d, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for all_test_embeddings and store it in y_pred\n",
    "y_pred = clf.predict(X_test_2d)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding-Based Gradient Boosting Classifier\n",
    "\n",
    "The Gradient Boosting Classifier, trained using word embeddings as features, achieved an impressive accuracy of 90% on the test data. The classification report provides a detailed breakdown of precision, recall, and F1-score for both classes (0: FAKE, 1: REAL).\n",
    "\n",
    "|              | Precision | Recall | F1-Score | Support |\n",
    "|--------------|-----------|--------|----------|---------|\n",
    "| Fake (0)      | 0.88      | 0.93   | 0.91     | 633     |\n",
    "| Real (1)      | 0.93      | 0.88   | 0.90     | 634     |\n",
    "| Accuracy     |           |        | 0.90     | 1267    |\n",
    "| Macro Avg    | 0.90      | 0.90   | 0.90     | 1267    |\n",
    "| Weighted Avg | 0.90      | 0.90   | 0.90     | 1267    |\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "#### Gradient Boosting with TF-IDF Vectorization (After Tuning): 90% Accuracy\n",
    "The Gradient Boosting Classifier, trained on TF-IDF vectorized data after hyperparameter tuning, achieved an accuracy of 90% on the test data. The model demonstrated balanced precision and recall for both classes.\n",
    "\n",
    "#### Gradient Boosting with Word Embeddings: 90% Accuracy\n",
    "The Word Embedding-based Gradient Boosting Classifier also achieved an accuracy of 90%, matching the performance of the tuned TF-IDF model. This result suggests that using word embeddings as features yields comparable performance to traditional TF-IDF vectorization.\n",
    "\n",
    "In summary, the word embedding-based model provides a viable alternative for text classification, showcasing its effectiveness in capturing semantic relationships and context within the text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning for word embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make some predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_news = [\n",
    "    \"Michigan governor denies misleading U.S. House on Flint water (Reuters) - Michigan Governor Rick Snyder denied Thursday that he had misled a U.S. House of Representatives committee last year over testimony on Flintâ€™s water crisis after lawmakers asked if his testimony had been contradicted by a witness in a court hearing. The House Oversight and Government Reform Committee wrote Snyder earlier Thursday asking him about published reports that one of his aides, Harvey Hollins, testified in a court hearing last week in Michigan that he had notified Snyder of an outbreak of Legionnairesâ€™ disease linked to the Flint water crisis in December 2015, rather than 2016 as Snyder had testified. â€œMy testimony was truthful and I stand by it,â€ Snyder told the committee in a letter, adding that his office has provided tens of thousands of pages of records to the committee and would continue to cooperate fully.  Last week, prosecutors in Michigan said Dr. Eden Wells, the stateâ€™s chief medical executive who already faced lesser charges, would become the sixth current or former official to face involuntary manslaughter charges in connection with the crisis. The charges stem from more than 80 cases of Legionnairesâ€™ disease and at least 12 deaths that were believed to be linked to the water in Flint after the city switched its source from Lake Huron to the Flint River in April 2014. Wells was among six current and former Michigan and Flint officials charged in June. The other five, including Michigan Health and Human Services Director Nick Lyon, were charged at the time with involuntary manslaughter\",\n",
    "    \" WATCH: Fox News Host Loses Her Sh*t, Says Investigating Russia For Hacking Our Election Is Unpatriotic This woman is insane.In an incredibly disrespectful rant against President Obama and anyone else who supports investigating Russian interference in our election, Fox News host Jeanine Pirro said that anybody who is against Donald Trump is anti-American. Look, it s time to take sides,  she began.\",\n",
    "    \" Sarah Palin Celebrates After White Man Who Pulled Gun On Black Protesters Goes Unpunished (VIDEO) Sarah Palin, one of the nigh-innumerable  deplorables  in Donald Trump s  basket,  almost outdid herself in terms of horribleness on Friday.\"\n",
    "]\n",
    "\n",
    "test_news_vectors = [preprocess_and_vectorize(n) for n in test_news]\n",
    "clf.predict(test_news_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppttUIGMNzk7"
   },
   "source": [
    "**Confusion Matrix for Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 150}\n",
      "Tuned Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92       633\n",
      "           1       0.93      0.90      0.92       634\n",
      "\n",
      "    accuracy                           0.92      1267\n",
      "   macro avg       0.92      0.92      0.92      1267\n",
      "weighted avg       0.92      0.92      0.92      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'learning_rate': [0.2, 0.3],\n",
    "    'max_depth': [5, 6],\n",
    "    # we can Add more hyperparameters to tune\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_2d, y_train)\n",
    "\n",
    "# Get the best parameters found\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_best = best_model.predict(X_test_2d)\n",
    "\n",
    "# Print the classification report for the tuned model\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Tuned Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned Gradient Boosting model with word embeddings achieved optimal performance with the following hyperparameters: learning rate of 0.3, max depth of 5, and 150 estimators. The classification report demonstrates balanced precision, recall, and an overall accuracy of 92% on the test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 250}\n",
      "Tuned Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       633\n",
      "           1       0.93      0.91      0.92       634\n",
      "\n",
      "    accuracy                           0.92      1267\n",
      "   macro avg       0.92      0.92      0.92      1267\n",
      "weighted avg       0.92      0.92      0.92      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [150,200,250],\n",
    "    'learning_rate': [0.3,.4,.5],\n",
    "    'max_depth': [3,4,5],\n",
    "    # we can Add more hyperparameters to tune\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_2d, y_train)\n",
    "\n",
    "# Get the best parameters found\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_best = best_model.predict(X_test_2d)\n",
    "\n",
    "# Print the classification report for the tuned model\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Tuned Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further  tuning Gradient Boosting model with word embeddings achieved optimal performance with the following hyperparameters: learning rate of 0.5, max depth of 5, and 250 estimators. The classification report demonstrates balanced precision, recall, and an overall accuracy of 92% on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(95.72222222222221, 0.5, 'Truth')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#finally print the confusion matrix for the best model (Tuning GradientBoostingClassifier)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "cm\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this text classification project, we explored three different approaches to model training: Multinomial Naive Bayes using TF-IDF vectorization, Gradient Boosting with TF-IDF vectorization, and Gradient Boosting with word embeddings. Each approach presented its own set of strengths and considerations.\n",
    "\n",
    "1. **Multinomial Naive Bayes (TF-IDF Vectorization):**\n",
    "   - Achieved an accuracy of ~84.8% before hyperparameter tuning and improved to ~89.3% after tuning.\n",
    "   - Notable misclassification tendencies with a higher number of false positives.\n",
    "\n",
    "2. **Gradient Boosting (TF-IDF Vectorization):**\n",
    "   - Achieved an accuracy of ~88% before hyperparameter tuning and improved to ~90% after tuning.\n",
    "   - Balanced precision and recall for both classes.\n",
    "\n",
    "3. **Word Embedding-Based Gradient Boosting:**\n",
    "   - Demonstrated an impressive accuracy of 90%, showcasing the effectiveness of word embeddings in capturing semantic relationships.\n",
    "\n",
    "4. **Tuned Word Embedding-Based Gradient Boosting:**\n",
    "   - Achieved the highest accuracy of 92% after hyperparameter tuning, outperforming all other models.\n",
    "   - Balanced precision and recall for both classes, indicating robust performance.\n",
    "\n",
    "### Recommendations for Model Improvement\n",
    "\n",
    "To further enhance model performance:\n",
    "\n",
    "- **Feature Engineering:** Experiment with additional text preprocessing techniques, such as stemming or handling special characters, to improve the quality of input features.\n",
    "\n",
    "- **Ensemble Methods:** Explore ensemble methods like stacking or blending multiple models to leverage their individual strengths.\n",
    "\n",
    "- **Advanced Embeddings:** Consider using more advanced word embeddings techniques, such as BERT embeddings, to capture richer semantic information in the text.\n",
    "\n",
    "- **Increase Data Volume:** Augment the dataset with more labeled examples to provide the models with a broader understanding of the text patterns.\n",
    "\n",
    "- **Deep Learning Approaches:** Explore deep learning architectures like recurrent neural networks (RNNs) or transformers (e.g., BERT) for text classification tasks. These models can capture intricate relationships within the text data.\n",
    "\n",
    "These enhancements can contribute to building more robust and accurate text classification models for various applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "301.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
